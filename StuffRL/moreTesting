from sb3_contrib import MaskablePPO
from stable_baselines3 import PPO
from sb3_contrib.common.envs import InvalidActionEnvDiscrete
from sb3_contrib.common.maskable.evaluation import evaluate_policy
from sb3_contrib.common.maskable.utils import get_action_masks
# This is a drop-in replacement for EvalCallback
import gymnasium as gym
import numpy as np
from sb3_contrib.common.maskable.callbacks import MaskableEvalCallback
from sb3_contrib.common.wrappers import ActionMasker
from GymEnv import ChineseCheckersBoard
env = ChineseCheckersBoard(6)
# def mask_fn(env: gym.Env) -> np.ndarray:
    # Do whatever you'd like in this function to return the action mask
    # for the current env. In this example, we assume the env has a
    # helpful method we can rely on.
    # return env.valid_action_mask()
# env = InvalidActionEnvDiscrete(dim=80,
# n_invalid_actions=60)
model = PPO("MultiInputPolicy", env, verbose=1)
model.learn(10000000)

# evaluate_policy(model, env, n_eval_episodes=20, warn=False)

model.save("ppo_mask")
# del model # remove to demonstrate saving and loading

# model = MaskablePPO.load("ppo_mask")

# obs, _ = env.reset()
# while True:
#     # Retrieve current action mask
#     action_masks = get_action_masks(env)
#     action, _states = model.predict(obs, action_masks=action_masks)
#     obs, reward, terminated, truncated, info = env.step(action)