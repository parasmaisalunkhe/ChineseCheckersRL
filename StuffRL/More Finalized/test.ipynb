{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5afd764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from termcolor import colored\n",
    "from gymnasium import spaces\n",
    "import gymnasium as gym\n",
    "width = 29\n",
    "height = 19\n",
    "\n",
    "class ChineseCheckersBoard(gym.Env):\n",
    "    def __init__(self, n_players):\n",
    "        super().__init__()\n",
    "        self.numPlayers = n_players\n",
    "        self.StartingPositions = [\n",
    "            [0,1,2,3,4,5,6,7,8,9],\n",
    "            [19,20,21,22,32,33,34,44,45,55],\n",
    "            [74,84,85,95,96,97,107,108,109,110],\n",
    "            [111,112,113,114,115,116,117,118,119,120],\n",
    "            [65,75,76,86,87,88,98,99,100,101],\n",
    "            [10,11,12,13,23,24,25,35,36,46],\n",
    "        ]\n",
    "        self.WinningPositions = [\n",
    "            [111,112,113,114,115,116,117,118,119,120],\n",
    "            [65,75,76,86,87,88,98,99,100,101],\n",
    "            [10,11,12,13,23,24,25,35,36,46],\n",
    "            [0,1,2,3,4,5,6,7,8,9],\n",
    "            [19,20,21,22,32,33,34,44,45,55],\n",
    "            [74,84,85,95,96,97,107,108,109,110],\n",
    "        ]\n",
    "        \n",
    "        self.StartingLocations = {2: [0,3], 3: [0,3,3], 4: [0,1,3,4], 6: [0,1,2,3,4,5]}.get(self.numPlayers)\n",
    "        # self.EndingLocations = {2: [3,0], 3: [3,5,1], 4: [3,4,1,2], 6: [3,4,5,0,1,2]}.get(self.numPlayers)\n",
    "        self.PlayerPOVPosition = {2: [3,3], 3: [2,2,2], 4: [1,2,1,2], 6: [1,1,1,1,1,1]}.get(self.numPlayers) #CCW\n",
    "        self.emptyTokenLocations = None\n",
    "        \n",
    "        self.GlobalBoard = self.ChineseCheckersPattern().astype(np.int32)\n",
    "        self.currentPlayerBoardView = self.GlobalBoard.copy()\n",
    "        self.ActualEndingLocations = [self.emptyTokenLocations[x] for x in self.WinningPositions[0]]\n",
    "\n",
    "        self.agents = self.possible_agents = [\"player_\" + str(r) for r in range(1,self.numPlayers+1)]\n",
    "        self.agentsID = {item: idx + 1 for idx, item in enumerate(self.agents)}\n",
    "        self.IDagents = {idx + 1: item for idx, item in enumerate(self.agents)}\n",
    "\n",
    "        self.current_player = None \n",
    "\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"obs\": spaces.Box(low=-1, high=2, shape=(121,), dtype=np.float64),\n",
    "            \"action_mask\": spaces.Sequence(spaces.MultiDiscrete([width*height, width*height])),\n",
    "            \"measurements\": spaces.Box(low=-np.inf, high=np.inf, shape=(4, 1), dtype=np.float32)\n",
    "        })\n",
    "        self.action_space = spaces.MultiDiscrete([width*height, width*height])\n",
    "\n",
    "        self.last_move = None\n",
    "        self.num_moves = 0\n",
    "    \n",
    "    def ChineseCheckersPattern(self):\n",
    "        Dict = {\"X\": 0, \".\": -1}\n",
    "        finalpattern = \".\" * width\n",
    "        holes = [1,2,3,4,13,12,11,10,9,10,11,12,13,4,3,2,1]  # holes = [1,2,3,4,5,6,7,8,9,8,7,6,5,4,3,2,1]\n",
    "        for n in holes:\n",
    "            pattern = \"\"\n",
    "            for i in range(n):\n",
    "                pattern += \"X.\"\n",
    "            pattern = pattern[:-1]\n",
    "            while len(pattern) != width:\n",
    "                pattern = \".\" + pattern + \".\"\n",
    "            finalpattern += pattern\n",
    "        finalpattern += \".\" * width\n",
    "        newBoard = np.array([Dict[char] for char in finalpattern])\n",
    "        self.emptyTokenLocations = list(np.where(newBoard == 0)[0])\n",
    "        for i,loc in enumerate(self.StartingLocations):\n",
    "            for x in self.StartingPositions[loc]:\n",
    "                newBoard[self.emptyTokenLocations[x]] = i+1\n",
    "        return newBoard.astype(dtype=np.float32)\n",
    "\n",
    "    def render(self, board):\n",
    "        \"\"\"Prints ASCII representations of the Global board.\"\"\"\n",
    "        board = board.astype(np.int32)\n",
    "        PlayertoColor = [\"black\", \"white\", \"yellow\", \"blue\", \"greennnn\", \"magenta\", \"cyan\", \"red\"]\n",
    "        for i in range(height):\n",
    "            row = \" \".join(colored(str(x) if x != -1 else \" \", PlayertoColor[x+1]) for x in board[i*width:(i+1)*width])\n",
    "            print(row)\n",
    "\n",
    "    def nextPlayerPOV(self):\n",
    "        board = None\n",
    "        rotationMove = self.PlayerPOVPosition.pop(0)\n",
    "        board = self.rotateNtimes(self.GlobalBoard, rotationMove)\n",
    "        self.PlayerPOVPosition.append(rotationMove)\n",
    "        return board\n",
    "\n",
    "    def rotate(self, board):\n",
    "        newboard = board.copy()\n",
    "        template = [10,23,11,35,24,12,46,36,25,13,98,86,75,65,56,47,37,26,14,6,3,1,0,99,87,76,66,57,48,38,27,15,7,4,2,100,88,77,67,58,49,39,28,16,8,5,101,89,78,68,59,50,40,29,17,9,102,90,79,69,60,51,41,30,18,111,103,91,80,70,61,52,42,31,19,115,112,104,92,81,71,62,53,43,32,20,118,116,113,105,93,82,72,63,54,44,33,21,120,119,117,114,106,94,83,73,64,55,45,34,22,107,95,84,74,108,96,85,109,97,110]\n",
    "        for i,x in enumerate(template):\n",
    "            newboard[self.emptyTokenLocations[x]] = board[self.emptyTokenLocations[i]]\n",
    "        return newboard\n",
    "    \n",
    "    def rotateNtimes(self, board, n):\n",
    "        for i in range(n):\n",
    "            board = self.rotate(board)\n",
    "        return board\n",
    "    \n",
    "    def allLegalActions(self, board, player_num):\n",
    "        legal_actions = []\n",
    "        for index,x in enumerate(board):\n",
    "            if x == player_num:\n",
    "                AllValidmoves = self.TheListofAllPossibleMoves(index, board)\n",
    "                tuples = [np.array([index, num]) for num in AllValidmoves]\n",
    "                legal_actions += tuples\n",
    "        return legal_actions\n",
    "\n",
    "    def jumpHelper(self, JumpsLegal, callStack, board):\n",
    "        newCallStack = callStack\n",
    "        LegalMoves = JumpsLegal\n",
    "        for index in LegalMoves:\n",
    "            possibleFurtherJumps = set()\n",
    "            posJumpMoves = [(index+2, index+4), (index-2, index-4), (index-width+1,index-2*width+2), (index-width-1, index-2*width-2), (index+width+1, index+2*width+2), (index+width-1, index+2*width-2)]\n",
    "            for mv in posJumpMoves:\n",
    "                if mv[0] > 0 and mv[0] < width*height and mv[1] > 0 and mv[1] < width*height and board[mv[0]] != 0 and board[mv[0]] != -1 and board[mv[1]] == 0:\n",
    "                    if mv[1] not in newCallStack:\n",
    "                        possibleFurtherJumps.add(mv[1])\n",
    "                        newCallStack.add(mv[1])\n",
    "            return list(LegalMoves) + list(self.jumpHelper(possibleFurtherJumps, newCallStack, board))\n",
    "        return list(LegalMoves)\n",
    "            \n",
    "    def TheListofAllPossibleMoves(self, index, board):\n",
    "        board = board.astype(np.int32)\n",
    "        possibleSteps = set()\n",
    "        possibleJumps = set()\n",
    "        posOneStepMoves = [(index+2, index+4), (index-2, index-4), (index-width+1,index-2*width+2), (index-width-1, index-2*width-2), (index+width+1, index+2*width+2), (index+width-1, index+2*width-2)]\n",
    "        for x in posOneStepMoves:\n",
    "            if x[0] > 0 and x[0] < width*height:\n",
    "                # print(board[index], board[x[0]])\n",
    "                if board[x[0]] == 0:\n",
    "                    # if index in self.ActualEndingLocations and x[0] in self.ActualEndingLocations:\n",
    "                    possibleSteps.add(x[0])\n",
    "                    # if index not in self.ActualEndingLocations:\n",
    "                    #     possibleSteps.add(x[0])\n",
    "                elif x[1] > 0 and x[1] < width*height and board[x[1]] == 0:\n",
    "                    # print(board[index], board[x[1]])\n",
    "                    # if index in self.ActualEndingLocations and x[1] in self.ActualEndingLocations:\n",
    "                    possibleSteps.add(x[1])\n",
    "                    # if index not in self.ActualEndingLocations:\n",
    "                        # possibleSteps.add(x[1])\n",
    "        if not possibleJumps:\n",
    "            return list(possibleSteps)\n",
    "        return list(possibleSteps) + list(self.jumpHelper(possibleJumps, set(), board))\n",
    "        \n",
    "    def isLegal(self, action, board, player_num):\n",
    "        lists = self.allLegalActions(board, player_num)\n",
    "        for x in lists:\n",
    "            if np.array_equal(x, action):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def isGameOver(self, board, player_num):\n",
    "        # print(\"moves:\", self.num_moves)\n",
    "        if self.num_moves <= 5:\n",
    "            return False\n",
    "        endLocation = [board[x] for x in self.ActualEndingLocations]\n",
    "        if len(set(endLocation)) == 1 and list(set(endLocation))[0] == player_num:\n",
    "            return True\n",
    "        else:\n",
    "            if 0 in endLocation:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "            \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        action = np.array(action)\n",
    "        # print(\"action\", action)\n",
    "        self.num_moves += 1\n",
    "        \n",
    "        reward = 0\n",
    "        # print(\"Key\", self.current_player)\n",
    "        board = self.GlobalBoard[:]\n",
    "        # print(self.IDagents[self.current_player])\n",
    "        if not self.isLegal(action, self.GlobalBoard, self.current_player):\n",
    "            # print(\"wat\")\n",
    "            reward = -5.0\n",
    "            done = True\n",
    "            boardObservation = self.getObservation(board)\n",
    "            validMoves = self.allLegalActions(self.GlobalBoard, self.current_player)\n",
    "            measures = self.getMeasures(board, self.current_player)\n",
    "            # nextActionMask = self.generate_action_mask(validMoves)\n",
    "            return {\"obs\": boardObservation, \"action_mask\": validMoves, \"measurements\": measures}, reward, done, False, {}\n",
    "        else:\n",
    "            Token = self.current_player\n",
    "            self.num_moves += 1\n",
    "            board[action[0]] = 0\n",
    "            board[action[1]] = Token\n",
    "            # endPlace = [board[x] for x in self.ActualEndingLocations]\n",
    "            # print(action[1], self.ActualEndingLocations)\n",
    "            if action[1] in self.ActualEndingLocations:\n",
    "                reward = 5.0\n",
    "            if action[0] in self.ActualEndingLocations and action[1] not in self.ActualEndingLocations:\n",
    "                reward = -2.0\n",
    "            if self.isGameOver(board, Token):\n",
    "                done = True\n",
    "                reward = 10.0\n",
    "            measures = self.getMeasures(board, self.current_player)\n",
    "            self.GlobalBoard = self.nextPlayerPOV()\n",
    "            self.next_player()\n",
    "            boardObservation = self.getObservation(board)\n",
    "            validMoves = self.allLegalActions(self.GlobalBoard, self.current_player)\n",
    "           \n",
    "            # nextActionMask = self.generate_action_mask(validMoves)\n",
    "            return {\"obs\": boardObservation, \"action_mask\": validMoves, \"measurements\": measures}, reward, done, False, {}\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        self.num_moves = 0\n",
    "        self.current_player = 1\n",
    "        self.GlobalBoard = self.ChineseCheckersPattern()\n",
    "        validMoves = self.allLegalActions(self.GlobalBoard, self.current_player)\n",
    "        board = self.GlobalBoard\n",
    "        self.num_moves = 0\n",
    "        boardObs = self.getObservation(board)\n",
    "        measure = self.getMeasures(self.GlobalBoard, self.current_player)\n",
    "        observation = {\"obs\": boardObs, \"action_mask\": validMoves, \"measurements\": measure}\n",
    "        return observation, {}\n",
    "    \n",
    "    def next_player(self):\n",
    "        self.current_player += 1\n",
    "        if self.current_player == self.numPlayers + 1:\n",
    "            self.current_player = 1\n",
    "\n",
    "    def getObservation(self, board):\n",
    "        board = board[board != -1]\n",
    "        newTarget = 3\n",
    "        target = self.current_player   # The specified number\n",
    "        board[board == target] = 9\n",
    "        mask = (board != 0) & (board != 9)\n",
    "        board[mask] = 2\n",
    "        board[board == 0] = 1\n",
    "        board[board == 9] = newTarget\n",
    "        return board\n",
    "        \n",
    "    def getMeasures(self, board, currentPlayer):\n",
    "        corner = 507\n",
    "        # print(self.ActualEndingLocations)\n",
    "        row = corner // width\n",
    "        col = corner % width\n",
    "        indices = np.where(board == currentPlayer)[0]\n",
    "        # print(indices)\n",
    "        rows = indices // width\n",
    "        cols = indices % width\n",
    "        indiciesCoordinates = np.stack((rows, cols), axis=1)\n",
    "        values = self.absolute_directional_euclidean_distance(indiciesCoordinates, (row, col))\n",
    "        newValues = list(values)\n",
    "        newValues.append(self.numPiecesEndZone(board))\n",
    "        return np.array(values, dtype=np.float32)\n",
    "    def numPiecesEndZone(self, board):\n",
    "        count = 0 \n",
    "        for x in self.ActualEndingLocations:\n",
    "            if board[x] != \".\":\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def absolute_directional_euclidean_distance(self, reference, points):\n",
    "        reference = np.array(reference)\n",
    "        points = np.array(points)\n",
    "\n",
    "        deltas = np.abs(points - reference)  # Absolute differences\n",
    "        avg_abs_delta = np.mean(deltas, axis=0)  # [avg_row_abs_delta, avg_col_abs_delta]\n",
    "\n",
    "        euclidean_dir_distance = np.linalg.norm(avg_abs_delta)\n",
    "        return avg_abs_delta[0], avg_abs_delta[1], euclidean_dir_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3d04f42",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ChineseCheckersBoard' object has no attribute 'board_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Neural Network model for policy + value estimation\u001b[39;00m\n\u001b[0;32m     24\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m obs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 25\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m (\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboard_size\u001b[49m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m build_policy_value_model(input_dim, action_dim)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Optimizer and hyperparameters\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ChineseCheckersBoard' object has no attribute 'board_size'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# ======= Chinese Checkers Neural Network Model (Policy + Value) =======\n",
    "def build_policy_value_model(input_dim, action_dim):\n",
    "    board_input = tf.keras.Input(shape=(input_dim,), name='board')\n",
    "    mask_input = tf.keras.Input(shape=(action_dim,), name='action_mask')\n",
    "\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(board_input)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    logits = tf.keras.layers.Dense(action_dim)(x)\n",
    "    masked_logits = tf.keras.layers.Multiply()([logits, mask_input])\n",
    "    value = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=[board_input, mask_input], outputs=[masked_logits, value])\n",
    "\n",
    "# Create environment\n",
    "env = ChineseCheckersBoard(2)\n",
    "num_steps_max = 10000000\n",
    "\n",
    "obs, info = env.reset()\n",
    "# Neural Network model for policy + value estimation\n",
    "input_dim = obs['obs'].shape[0]\n",
    "action_dim = (env.board_size ** 2) ** 2\n",
    "model = build_policy_value_model(input_dim, action_dim)\n",
    "\n",
    "# Optimizer and hyperparameters\n",
    "gamma = 0.99\n",
    "value_coef = 0.5\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# ======= Main Training Loop =======\n",
    "def evaluate_board(board):\n",
    "    # Placeholder for board evaluation logic (heuristics)\n",
    "    # This could be any evaluation function that returns a score between 0 and 1\n",
    "    # For simplicity, assume a simple evaluation function that returns a score based on the board configuration\n",
    "    return np.random.rand()  # Example score, you should replace it with an actual evaluation\n",
    "\n",
    "# Function to compute returns\n",
    "def compute_returns(rewards, gamma=0.99):\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    return np.array(returns, dtype=np.float32)\n",
    "\n",
    "for episode in range(num_steps_max):\n",
    "    done = False\n",
    "    states, actions, rewards, log_probs, values = [], [], [], [], []\n",
    "\n",
    "    while not done:\n",
    "        board = tf.convert_to_tensor([obs[\"board\"]], dtype=tf.float32)\n",
    "        mask = tf.convert_to_tensor([obs[\"action_mask\"]], dtype=tf.float32)\n",
    "\n",
    "        # Get action probabilities from policy\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits, value = model([board, mask])\n",
    "            masked_logits = tf.where(mask == 1, logits, tf.float32.min)\n",
    "            action_probs = tf.nn.softmax(masked_logits)\n",
    "            action_dist = tf.random.categorical(tf.math.log(action_probs), num_samples=1)\n",
    "            action = int(action_dist[0][0])\n",
    "\n",
    "            log_prob = tf.math.log(action_probs[0, action] + 1e-8)\n",
    "            value = tf.squeeze(value)\n",
    "\n",
    "        # Store transition for the current action\n",
    "        states.append(board)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "\n",
    "        # Evaluate the current state of the board\n",
    "        current_score = evaluate_board(obs['board'])\n",
    "        \n",
    "        # Find the best action based on evaluating each potential new board\n",
    "        best_action = None\n",
    "        best_score = -np.inf\n",
    "\n",
    "        for action in obs[\"action_mask\"]:\n",
    "            backup_board = env.GlobalBoard.copy()\n",
    "            backup_player = env.current_player\n",
    "            backup_num_moves = env.num_moves\n",
    "\n",
    "            try:\n",
    "                temp_obs, reward, done, truncated, info = env.step(action)\n",
    "                temp_score = evaluate_board(temp_obs[\"board\"])\n",
    "\n",
    "                if temp_score > best_score:\n",
    "                    best_score = temp_score\n",
    "                    best_action = action\n",
    "\n",
    "                env.GlobalBoard = backup_board\n",
    "                env.current_player = backup_player\n",
    "                env.num_moves = backup_num_moves\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Invalid action {action} caused exception: {e}\")\n",
    "                env.GlobalBoard = backup_board\n",
    "                env.current_player = backup_player\n",
    "                env.num_moves = backup_num_moves\n",
    "\n",
    "        if best_action is not None:\n",
    "            obs, reward, done, truncated, info = env.step(best_action)\n",
    "        else:\n",
    "            print(\"No valid action found, breaking.\")\n",
    "            break\n",
    "\n",
    "        # End the episode if done\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Post-episode processing\n",
    "    returns = compute_returns(rewards, gamma)\n",
    "    returns = tf.convert_to_tensor(returns, dtype=tf.float32)\n",
    "    log_probs = tf.stack(log_probs)\n",
    "    values = tf.stack(values)\n",
    "    advantages = returns - values\n",
    "\n",
    "    # Compute losses\n",
    "    policy_loss = -tf.reduce_mean(log_probs * advantages)\n",
    "    value_loss = tf.reduce_mean(tf.square(returns - values))\n",
    "    total_loss = policy_loss + value_coef * value_loss\n",
    "\n",
    "    # Apply gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        total_loss\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Reward = {sum(rewards)}, Loss = {total_loss.numpy():.4f}\")\n",
    "    \n",
    "    # Rendering (visualize board)\n",
    "    env.render(env.GlobalBoard)\n",
    "    input()\n",
    "    print(\"\\033[H\\033[J\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1422d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Senior",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
