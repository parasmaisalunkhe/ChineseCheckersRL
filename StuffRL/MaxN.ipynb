{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc10ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "\n",
    "def create_value_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(121,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)  # Predict single value\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d29a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_value_model(env_class, num_players=3, episodes=1000, search_depth=2):\n",
    "    model = create_value_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    batch_size = 64\n",
    "    replay_buffer = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        env = env_class()\n",
    "        game_memory = []\n",
    "\n",
    "        while not env.is_terminal():\n",
    "            player_id = env.get_current_player()\n",
    "            obs = env.get_observation(player_id)\n",
    "\n",
    "            _, move = maxn_alpha_beta_nn(\n",
    "                env,\n",
    "                depth=0,\n",
    "                max_depth=search_depth,\n",
    "                num_players=num_players,\n",
    "                value_net=model,\n",
    "                alpha=np.full(num_players, -np.inf),\n",
    "                beta=np.full(num_players, np.inf)\n",
    "            )\n",
    "\n",
    "            game_memory.append((obs, player_id))\n",
    "            env.apply_move(player_id, move)\n",
    "\n",
    "        final_scores = env.get_final_scores()\n",
    "        for obs, pid in game_memory:\n",
    "            replay_buffer.append((obs, final_scores[pid]))\n",
    "\n",
    "        if len(replay_buffer) > 5000:\n",
    "            replay_buffer = replay_buffer[-5000:]\n",
    "\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            import random\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            batch_obs = tf.convert_to_tensor([b[0] for b in batch], dtype=tf.float32)\n",
    "            batch_targets = tf.convert_to_tensor([b[1] for b in batch], dtype=tf.float32)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                preds = tf.squeeze(model(batch_obs), axis=1)\n",
    "                loss = tf.reduce_mean(tf.square(preds - batch_targets))\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            if episode % 50 == 0:\n",
    "                print(f\"Episode {episode} | Training loss: {loss.numpy():.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e314e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_game(env_class, value_net, search_depth=2, num_players=3, render=False):\n",
    "    env = env_class()\n",
    "    move_count = 0\n",
    "\n",
    "    while not env.is_terminal():\n",
    "        player_id = env.get_current_player()\n",
    "        obs = env.get_observation(player_id)\n",
    "\n",
    "        _, move = maxn_alpha_beta_nn(\n",
    "            env,\n",
    "            depth=0,\n",
    "            max_depth=search_depth,\n",
    "            num_players=num_players,\n",
    "            value_net=value_net,\n",
    "            alpha=np.full(num_players, -np.inf),\n",
    "            beta=np.full(num_players, np.inf)\n",
    "        )\n",
    "\n",
    "        if render:\n",
    "            print(f\"\\nPlayer {player_id} plays {move}\")\n",
    "            env.render()  # Optional: only if your env supports it\n",
    "\n",
    "        env.apply_move(player_id, move)\n",
    "        move_count += 1\n",
    "\n",
    "    final_scores = env.get_final_scores()\n",
    "    print(\"\\nðŸŽ‰ Game Over\")\n",
    "    for pid, score in enumerate(final_scores):\n",
    "        print(f\"Player {pid}: Score = {score}\")\n",
    "    print(f\"Total Moves: {move_count}\")\n",
    "    return final_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb679a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = train_value_model(env_class=YourChineseCheckersEnv)\n",
    "\n",
    "run_inference_game(env_class=YourChineseCheckersEnv, value_net=trained_model, search_depth=2, render=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Senior",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
